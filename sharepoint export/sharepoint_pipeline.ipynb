{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SharePoint Export Pipeline\n",
    "\n",
    "ETL pipeline that:\n",
    "1. Extracts data from SharePoint CSV and Salesforce Excel exports\n",
    "2. Transforms data with cleaning, product explosion, and metrics calculation\n",
    "3. Loads data into Snowflake with incremental or full refresh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Import product mappings from separate file\n",
    "from product_mappings import PRODUCT_CONFIGS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SNOWFLAKE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    'account': \"uhgdwaas.east-us-2.azure\",\n",
    "    'user': os.getenv('SF_USERNAME'),\n",
    "    'password': os.getenv('SF_PW'),\n",
    "    'role': \"AZU_SDRP_CSZNB_PRD_DEVELOPER_ROLE\",\n",
    "    'warehouse': \"CSZNB_PRD_ANALYTICS_XS_WH\",\n",
    "    'database': 'CSZNB_PRD_OA_DEV_DB',\n",
    "    'schema': 'BASE'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE NAMES\n",
    "# ============================================================================\n",
    "\n",
    "SOURCE_TABLE = 'SHAREPOINT_ANALYTIC_REQUESTS'\n",
    "TARGET_TABLE = 'FOCUSED_ANALYTIC_REQUESTS'\n",
    "SALESFORCE_TABLE = 'SALESFORCE_INITIATIVES'\n",
    "\n",
    "# ============================================================================\n",
    "# FILE PATHS\n",
    "# ============================================================================\n",
    "\n",
    "SHAREPOINT_EXPORT_PATH = Path.home() / \"Library/CloudStorage/OneDrive-UHG/Projects/SharePoint/exports/sharepoint_requests.csv\"\n",
    "SALESFORCE_EXPORT_PATH = Path.home() / \"Library/CloudStorage/OneDrive-UHG/Projects/SharePoint/exports/salesforce_exports.xlsx\"\n",
    "\n",
    "# ============================================================================\n",
    "# BUSINESS LOGIC CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "OPEN_STATUS = ['Not Started', 'In Progress', 'Waiting']\n",
    "DAYS_ON_STATUS_THRESHOLD = 14\n",
    "\n",
    "# Client type mapping\n",
    "CLIENT_TYPE_MAPPING = {\n",
    "    '1': 'Optum Direct NBEA',\n",
    "    '2': 'Optum/UHC Cross Carrier NBEA',\n",
    "    '3': 'UHC NBEA',\n",
    "    '4': 'Opum Direct',\n",
    "    '5': 'UHC Cross Carrier',\n",
    "    '6': 'Prospective',\n",
    "    '7': 'N/A',\n",
    "    '8': 'N/A'\n",
    "}\n",
    "\n",
    "# Boolean columns\n",
    "BOOLEAN_COLUMNS = [\n",
    "    \"BARIATRIC\", \"BH\", \"CGP\", \"CSP\", \"DM\", \"KIDNEY\", \"TRANSPLANT\", \"CHD\", \"VAD\",\n",
    "    \"NICU\", \"MATERNITY\", \"FERTILITY\", \"FOCUSED_ANALYTICS\", \"OUTPATIENT_REHAB\",\n",
    "    \"OHS\", \"FCR_PROFESSIONAL\", \"CKS\", \"CKD\", \"CARDIOLOGY\", \"DME\", \"INPATIENT_REHAB\",\n",
    "    \"SPINE_PAIN_JOINT\", \"SPECIALTY_REDIRECTION\", \"MEDICAL_REBATES_ONBOARDING\",\n",
    "    \"BRS\", \"DATA_INTAKE\", \"DATA_QAVC\", \"SPECIALTY_FUSION\", \"MBO_IMPLEMENTATION\",\n",
    "    \"MSPN_IMPLEMENTATION\", \"VARIABLE_COPAY\", \"ACCUMULATOR_ADJUSTMENT\",\n",
    "    \"SMGP\", \"SGP\", \"SECOND_MD\", \"KAIA\", \"MBO_PRESALES\", \"MSPN_PRESALES\",\n",
    "    \"MEDICAL_REBATES_PREDEAL\", \"MAVEN\", \"CAR_REPORT\", \"MSK_MSS\",\n",
    "    \"FCR_FACILITY\", \"RADIATION_ONCOLOGY\", \"VIRTA_HEALTH\", \"SMO_PRESALES\",\n",
    "    \"SMO_IMPLEMENTATION\", \"SBO_HEALTH_TRUST_PRESALES\", \"SBO_HEALTH_TRUST_IMPLEMENTATION\",\n",
    "    \"CORE_SBO\", \"ENHANCE_SBO\", \"OPTUM_GUIDE\", \"CYLINDER_HEALTH\", \"RESOURCE_BRIDGE\",\n",
    "    \"PHS\", \"CANCER\", \"PODIMETRICS\", \"CAR-T\", \"HELLO_HEART\", \"PHARMACY_GROWTH_PRESALE\", \"PHARMACY_GROWTH_EXISTING\"\n",
    "]\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  - Snowflake database: {SNOWFLAKE_CONFIG['database']}\")\n",
    "print(f\"  - Product configs: {len(PRODUCT_CONFIGS)} products\")\n",
    "print(f\"  - Boolean columns: {len(BOOLEAN_COLUMNS)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sharepoint(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load SharePoint CSV and return normalized DataFrame\"\"\"\n",
    "    logger.info(f\"Loading SharePoint export from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Use low_memory=False to prevent mixed type warnings\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"SharePoint file not found at {file_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.upper()\n",
    "    \n",
    "    logger.info(f\"Loaded {len(df)} SharePoint records\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_salesforce(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Salesforce Excel export with improved error handling.\n",
    "    Returns empty DataFrame if file missing or has issues.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading Salesforce export from {file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Read Excel file (assuming first sheet)\n",
    "        df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            logger.warning(\"Salesforce file is empty\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.upper()\n",
    "\n",
    "        # Convert any Excel date serial numbers to proper dates\n",
    "        for col in df.columns:\n",
    "            # Check if column might be dates (ends with _DATE or contains 'DATE')\n",
    "            if 'DATE' in col.upper():\n",
    "                try:\n",
    "                    # Try to convert Excel serial dates\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        logger.info(f\"Loaded {len(df)} Salesforce records with {len(df.columns)} columns\")\n",
    "        logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Salesforce file not found at {file_path} - skipping Salesforce load\")\n",
    "        return pd.DataFrame()  # Return completely empty DataFrame\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading Salesforce file: {e}\")\n",
    "        return pd.DataFrame()  # Return completely empty DataFrame\n",
    "\n",
    "\n",
    "print(\"✓ Extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and normalize raw data\"\"\"\n",
    "    logger.info(\"Cleaning and normalizing data...\")\n",
    "    \n",
    "    # Map client types\n",
    "    if 'CLIENT_TYPE_DETAIL' in df.columns:\n",
    "        df['CLIENT_TYPE_DETAIL'] = (\n",
    "            df['CLIENT_TYPE_DETAIL']\n",
    "            .astype(str)\n",
    "            .map(CLIENT_TYPE_MAPPING)\n",
    "            .fillna(df['CLIENT_TYPE_DETAIL'])\n",
    "        )\n",
    "    \n",
    "    # Fill null values in boolean columns (only those that exist)\n",
    "    existing_bool_cols = [col for col in BOOLEAN_COLUMNS if col in df.columns]\n",
    "    if existing_bool_cols:\n",
    "        df[existing_bool_cols] = df[existing_bool_cols].fillna(False)\n",
    "    \n",
    "    # Populate PRODUCTS_REQUESTED from boolean columns where null (vectorized approach)\n",
    "    if 'PRODUCTS_REQUESTED' in df.columns and existing_bool_cols:\n",
    "        mask_null = df['PRODUCTS_REQUESTED'].isnull()\n",
    "        if mask_null.any():\n",
    "            # Vectorized approach: create list of selected column names for each row\n",
    "            df.loc[mask_null, 'PRODUCTS_REQUESTED'] = (\n",
    "                df.loc[mask_null, existing_bool_cols]\n",
    "                .apply(lambda row: ', '.join(row.index[row].str.title()) if row.any() else 'None', axis=1)\n",
    "            )\n",
    "    \n",
    "    logger.info(\"Data cleaning complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Cleaning function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_products(df: pd.DataFrame, df_salesforce: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform wide-format data into product-level records\"\"\"\n",
    "    logger.info(\"Starting product transformation...\")\n",
    "    \n",
    "    # Explode products into separate rows\n",
    "    df_exploded = _explode_products(df)\n",
    "    \n",
    "    # Enrich with Salesforce data\n",
    "    df_enriched = _enrich_with_salesforce(df_exploded, df_salesforce)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    df_enriched = _calculate_metrics(df_enriched)\n",
    "    \n",
    "    logger.info(f\"Transformation complete: {len(df_exploded)} product-level records created\")\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "def _get_column_value(row, possible_names):\n",
    "    \"\"\"Helper to get column value trying multiple possible column names\"\"\"\n",
    "    for name in possible_names:\n",
    "        if name in row.index:\n",
    "            return row[name]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _explode_products(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Explode wide-format data into product-level records with flexible column mapping\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Log ALL columns for debugging\n",
    "    logger.info(f\"Available columns in source data ({len(df.columns)} total):\")\n",
    "    logger.info(f\"  {df.columns.tolist()}\")\n",
    "    \n",
    "    # Define flexible column mappings (multiple possible names for each field)\n",
    "    COLUMN_MAPPINGS = {\n",
    "        'ID': ['ID', 'REQUEST_ID'],\n",
    "        'TITLE': ['TITLE', 'REQUEST_TITLE', 'NAME'],\n",
    "        'REQUEST_DATE': ['REQUEST_DATE', 'REQUESTDATE', 'CREATED', 'CREATED_DATE'],\n",
    "        'CLIENT': ['CLIENT', 'CLIENT_NAME', 'CUSTOMER'],\n",
    "        'MARKET': ['MARKET', 'MARKET_NAME'],\n",
    "        'REQUESTOR': ['REQUESTOR', 'REQUESTER', 'REQUESTED_BY'],\n",
    "        'CLIENT_TYPE_DETAIL': ['CLIENT_TYPE_DETAIL', 'CLIENT_TYPE', 'CLIENTTYPE'],\n",
    "        'OVERALL_STATUS': ['OVERALL_STATUS', 'STATUS', 'REQUEST_STATUS', 'OVERALLSTATUS'],\n",
    "        'PRODUCTS_REQUESTED': ['PRODUCTS_REQUESTED', 'PRODUCTS', 'PRODUCTSREQUESTED'],\n",
    "        'SALESFORCE_ID': ['SALESFORCE_ID', 'SALESFORCEID', 'SF_ID'],\n",
    "        'STATUS_CHANGE_DATE': ['STATUS_CHANGE_DATE', 'STATUSCHANGEDATE', 'MODIFIED', 'MODIFIED_DATE'],\n",
    "        'CLOSED_DATE': ['CLOSED_DATE', 'CLOSEDDATE', 'DATE_CLOSED'],\n",
    "        'PTRR': ['PTRR']\n",
    "    }\n",
    "    \n",
    "    # Check which columns are actually present\n",
    "    for field_name, possible_cols in COLUMN_MAPPINGS.items():\n",
    "        found = [col for col in possible_cols if col in df.columns]\n",
    "        if found:\n",
    "            logger.info(f\"  ✓ {field_name} mapped to: {found[0]}\")\n",
    "        else:\n",
    "            logger.warning(f\"  ✗ {field_name} NOT FOUND (tried: {possible_cols})\")\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        for product_name, category, field, start_col, end_col, status_col in PRODUCT_CONFIGS:\n",
    "            # Check if this product is requested\n",
    "            if field in row.index and row[field]:\n",
    "                record = {\n",
    "                    'ID': _get_column_value(row, COLUMN_MAPPINGS['ID']),\n",
    "                    'TITLE': _get_column_value(row, COLUMN_MAPPINGS['TITLE']),\n",
    "                    'REQUEST_DATE': _get_column_value(row, COLUMN_MAPPINGS['REQUEST_DATE']),\n",
    "                    'CLIENT': _get_column_value(row, COLUMN_MAPPINGS['CLIENT']),\n",
    "                    'MARKET': _get_column_value(row, COLUMN_MAPPINGS['MARKET']),\n",
    "                    'REQUESTOR': _get_column_value(row, COLUMN_MAPPINGS['REQUESTOR']),\n",
    "                    'CLIENT_TYPE': _get_column_value(row, COLUMN_MAPPINGS['CLIENT_TYPE_DETAIL']),\n",
    "                    'OVERALL_STATUS': _get_column_value(row, COLUMN_MAPPINGS['OVERALL_STATUS']),\n",
    "                    'PRODUCTS_REQUESTED': _get_column_value(row, COLUMN_MAPPINGS['PRODUCTS_REQUESTED']),\n",
    "                    'SALESFORCE_ID': _get_column_value(row, COLUMN_MAPPINGS['SALESFORCE_ID']),\n",
    "                    'PRODUCT': product_name,\n",
    "                    'PRODUCT_CATEGORY': category,\n",
    "                    'START_DATE': row.get(start_col),\n",
    "                    'COMPLETE_DATE': row.get(end_col),\n",
    "                    'STATUS': row.get(status_col),\n",
    "                    'STATUS_CHANGE_DATE': _get_column_value(row, COLUMN_MAPPINGS['STATUS_CHANGE_DATE']),\n",
    "                    'CLOSED_DATE': _get_column_value(row, COLUMN_MAPPINGS['CLOSED_DATE']),\n",
    "                    'PTRR': _get_column_value(row, COLUMN_MAPPINGS['PTRR'])\n",
    "                }\n",
    "                records.append(record)\n",
    "    \n",
    "    df_products = pd.DataFrame(records)\n",
    "    logger.info(f\"Exploded {len(df)} requests into {len(df_products)} product records\")\n",
    "    \n",
    "    # Log sample of first record for debugging\n",
    "    if len(df_products) > 0:\n",
    "        logger.info(f\"Sample transformed record:\")\n",
    "        logger.info(f\"  ID: {df_products.iloc[0]['ID']}\")\n",
    "        logger.info(f\"  TITLE: {df_products.iloc[0]['TITLE']}\")\n",
    "        logger.info(f\"  CLIENT: {df_products.iloc[0]['CLIENT']}\")\n",
    "        logger.info(f\"  OVERALL_STATUS: {df_products.iloc[0]['OVERALL_STATUS']}\")\n",
    "        logger.info(f\"  PRODUCT: {df_products.iloc[0]['PRODUCT']}\")\n",
    "    \n",
    "    return df_products\n",
    "\n",
    "\n",
    "def _enrich_with_salesforce(df: pd.DataFrame, df_salesforce: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join with Salesforce data to enrich records\"\"\"\n",
    "    if df_salesforce is None or df_salesforce.empty:\n",
    "        logger.warning(\"No Salesforce data available for enrichment\")\n",
    "        return df\n",
    "    \n",
    "    # Normalize Salesforce column names\n",
    "    df_salesforce.columns = df_salesforce.columns.str.upper()\n",
    "    \n",
    "    # Merge on SALESFORCE_ID if available\n",
    "    if 'SALESFORCE_ID' in df.columns and 'SALESFORCE_ID' in df_salesforce.columns:\n",
    "        df = df.merge(\n",
    "            df_salesforce[['SALESFORCE_ID', 'HAS_VALUE']],\n",
    "            on='SALESFORCE_ID',\n",
    "            how='left'\n",
    "        )\n",
    "        logger.info(\"Enriched with Salesforce data\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _calculate_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate derived metrics\"\"\"\n",
    "    today = pd.Timestamp.now()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['REQUEST_DATE', 'START_DATE', 'COMPLETE_DATE',\n",
    "                   'STATUS_CHANGE_DATE', 'CLOSED_DATE']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate days open\n",
    "    if 'REQUEST_DATE' in df.columns:\n",
    "        df['DAYS_OPEN'] = (today - df['REQUEST_DATE']).dt.days\n",
    "    \n",
    "    # Calculate product TAT (turnaround time)\n",
    "    if 'COMPLETE_DATE' in df.columns and 'START_DATE' in df.columns:\n",
    "        df['PRODUCT_TAT'] = (df['COMPLETE_DATE'] - df['START_DATE']).dt.days\n",
    "    \n",
    "    # Mark completed products\n",
    "    if 'STATUS' in df.columns:\n",
    "        df['COMPLETED_PRODUCT'] = df['STATUS'].isin(['Complete', 'Completed'])\n",
    "    \n",
    "    # Extract request type and year\n",
    "    if 'TITLE' in df.columns:\n",
    "        df['REQUEST_TYPE'] = df['TITLE'].str.extract(r'\\[(.*?)\\]')[0]\n",
    "    if 'REQUEST_DATE' in df.columns:\n",
    "        df['REQUEST_YEAR'] = df['REQUEST_DATE'].dt.year\n",
    "    \n",
    "    # Determine if product is open\n",
    "    if 'STATUS' in df.columns:\n",
    "        df['PRODUCT_OPEN'] = df['STATUS'].isin(OPEN_STATUS)\n",
    "    \n",
    "    # Calculate days on current status\n",
    "    if 'STATUS_CHANGE_DATE' in df.columns:\n",
    "        df['DAYS_ON_STATUS'] = (today - df['STATUS_CHANGE_DATE']).dt.days\n",
    "        df['DAYS_ON_STATUS'] = df['DAYS_ON_STATUS'].fillna(0).astype(int)\n",
    "    \n",
    "    # Flag items needing attention (open and on status > threshold)\n",
    "    if 'PRODUCT_OPEN' in df.columns and 'DAYS_ON_STATUS' in df.columns:\n",
    "        df['NEEDS_ATTENTION'] = (\n",
    "            df['PRODUCT_OPEN'] &\n",
    "            (df['DAYS_ON_STATUS'] > DAYS_ON_STATUS_THRESHOLD)\n",
    "        )\n",
    "    \n",
    "    # Add HAS_VALUE if not present\n",
    "    if 'HAS_VALUE' not in df.columns:\n",
    "        df['HAS_VALUE'] = None\n",
    "    \n",
    "    # Generate SharePoint URL\n",
    "    if 'ID' in df.columns:\n",
    "        df['URL'] = df['ID'].apply(\n",
    "            lambda x: f\"https://sharepoint.com/sites/analytics/Lists/Requests/DispForm.aspx?ID={x}\"\n",
    "            if pd.notna(x) else None\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Calculated all metrics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Transformation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Snowflake Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_connection():\n",
    "    \"\"\"Establish connection to Snowflake\"\"\"\n",
    "    logger.info(\"Connecting to Snowflake...\")\n",
    "    \n",
    "    # Optional: Load private key for key-pair authentication\n",
    "    pkey_pem = os.getenv(\"MY_SF_PKEY\")\n",
    "    pkey = None\n",
    "    if pkey_pem:\n",
    "        pkey = serialization.load_pem_private_key(\n",
    "            pkey_pem.encode(\"utf-8\"),\n",
    "            password=None,\n",
    "            backend=default_backend()\n",
    "        )\n",
    "    \n",
    "    conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
    "    logger.info(\"Successfully connected to Snowflake\")\n",
    "    return conn\n",
    "\n",
    "\n",
    "def normalize_dates(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert datetime columns to string format 'YYYY-MM-DD' for reliable Snowflake DATE parsing.\n",
    "    Returns: (normalized_dataframe, list_of_date_columns)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    date_columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Check if column name ends with _DATE or contains DATE-related keywords\n",
    "        if col.endswith('_DATE') or col in ['REQUEST_DATE', 'START_DATE', 'COMPLETE_DATE',\n",
    "                                               'CLOSED_DATE', 'STATUS_CHANGE_DATE']:\n",
    "            try:\n",
    "                # Convert to datetime64 first (handles strings, floats, NaT, etc.)\n",
    "                temp_dt = pd.to_datetime(df[col], errors='coerce')\n",
    "                \n",
    "                # Convert to string format 'YYYY-MM-DD', keeping NaT as None\n",
    "                df[col] = temp_dt.apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else None)\n",
    "                \n",
    "                # Explicitly cast to object dtype to ensure write_pandas treats as VARCHAR\n",
    "                df[col] = df[col].astype('object')\n",
    "                \n",
    "                date_columns.append(col)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not convert {col} to date string: {e}\")\n",
    "    \n",
    "    if date_columns:\n",
    "        logger.info(f\"Normalized {len(date_columns)} date columns to string format\")\n",
    "    \n",
    "    return df, date_columns\n",
    "\n",
    "\n",
    "def create_table_with_types(conn, table_name: str, df: pd.DataFrame, date_columns: List[str]):\n",
    "    \"\"\"\n",
    "    Create table with explicit DATE column types for date columns.\n",
    "    Numeric columns created as NUMBER, all other columns as VARCHAR.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    database = SNOWFLAKE_CONFIG['database']\n",
    "    schema = SNOWFLAKE_CONFIG['schema']\n",
    "    \n",
    "    # Build column definitions\n",
    "    column_defs = []\n",
    "    for col in df.columns:\n",
    "        if col in date_columns:\n",
    "            column_defs.append(f\"{col} DATE\")\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check if it's an integer or float\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                column_defs.append(f\"{col} NUMBER(38,0)\")\n",
    "            else:\n",
    "                column_defs.append(f\"{col} NUMBER(38,6)\")\n",
    "        else:\n",
    "            # Use VARCHAR for all other columns\n",
    "            column_defs.append(f\"{col} VARCHAR\")\n",
    "    \n",
    "    columns_sql = \",\\n    \".join(column_defs)\n",
    "    \n",
    "    create_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database}.{schema}.{table_name} (\n",
    "        {columns_sql}\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Creating table {table_name} with {len(date_columns)} DATE columns...\")\n",
    "    \n",
    "    try:\n",
    "        cur.execute(create_sql)\n",
    "        logger.info(f\"Successfully created table {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Table creation failed (may already exist): {e}\")\n",
    "    \n",
    "    cur.close()\n",
    "\n",
    "\n",
    "def ensure_schema_matches(conn, table_name: str, df: pd.DataFrame, date_columns: List[str]):\n",
    "    \"\"\"\n",
    "    Add any missing columns to existing table (schema evolution).\n",
    "    This allows the pipeline to handle new columns from SharePoint CSV automatically.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    database = SNOWFLAKE_CONFIG['database']\n",
    "    schema = SNOWFLAKE_CONFIG['schema']\n",
    "    \n",
    "    try:\n",
    "        # Get existing columns from Snowflake table\n",
    "        cur.execute(f\"SHOW COLUMNS IN TABLE {database}.{schema}.{table_name}\")\n",
    "        existing_cols = {row[2].upper() for row in cur.fetchall()}  # row[2] is column name\n",
    "        \n",
    "        # Find new columns in DataFrame that don't exist in table\n",
    "        df_cols = set(df.columns)\n",
    "        new_cols = df_cols - existing_cols\n",
    "        \n",
    "        if new_cols:\n",
    "            logger.info(f\"Schema evolution: Found {len(new_cols)} new columns to add\")\n",
    "            for col in sorted(new_cols):\n",
    "                # Determine column type based on data\n",
    "                if col in date_columns:\n",
    "                    col_type = \"DATE\"\n",
    "                elif pd.api.types.is_integer_dtype(df[col]):\n",
    "                    col_type = \"NUMBER(38,0)\"\n",
    "                elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    col_type = \"NUMBER(38,6)\"\n",
    "                else:\n",
    "                    col_type = \"VARCHAR\"\n",
    "                \n",
    "                alter_sql = f\"ALTER TABLE {database}.{schema}.{table_name} ADD COLUMN {col} {col_type}\"\n",
    "                logger.info(f\"  Adding column: {col} ({col_type})\")\n",
    "                cur.execute(alter_sql)\n",
    "            \n",
    "            logger.info(f\"Successfully added {len(new_cols)} new columns\")\n",
    "        else:\n",
    "            logger.info(f\"Schema matches - no new columns to add\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If table doesn't exist, this will fail but that's okay\n",
    "        # create_table_with_types will handle creation\n",
    "        logger.debug(f\"Could not check schema (table may not exist yet): {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "\n",
    "print(\"✓ Snowflake connection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "import uuid\n\n# Audit table name\nAUDIT_TABLE = 'PIPELINE_RUN_HISTORY'\n\n\ndef create_audit_table(conn):\n    \"\"\"\n    Create the audit table if it doesn't exist.\n    This table tracks all pipeline runs with detailed statistics.\n    \"\"\"\n    cur = conn.cursor()\n    \n    database = SNOWFLAKE_CONFIG['database']\n    schema = SNOWFLAKE_CONFIG['schema']\n    \n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {database}.{schema}.{AUDIT_TABLE} (\n        RUN_ID VARCHAR PRIMARY KEY,\n        RUN_TIMESTAMP TIMESTAMP,\n        PIPELINE_NAME VARCHAR,\n        TABLE_NAME VARCHAR,\n        LOAD_TYPE VARCHAR,\n        ROWS_PROCESSED NUMBER(38,0),\n        ROWS_INSERTED NUMBER(38,0),\n        ROWS_UPDATED NUMBER(38,0),\n        ROWS_DELETED NUMBER(38,0),\n        DURATION_SECONDS NUMBER(38,2),\n        STATUS VARCHAR,\n        ERROR_MESSAGE VARCHAR\n    );\n    \"\"\"\n    \n    try:\n        cur.execute(create_sql)\n        logger.info(f\"Ensured audit table {AUDIT_TABLE} exists\")\n    except Exception as e:\n        logger.warning(f\"Could not create audit table: {e}\")\n    finally:\n        cur.close()\n\n\ndef log_to_snowflake(conn, run_id: str, table_name: str, load_type: str, \n                     rows_processed: int, rows_inserted: int, rows_updated: int,\n                     duration_seconds: float, status: str, error_message: str = None):\n    \"\"\"\n    Log pipeline run statistics to Snowflake audit table.\n    \n    Args:\n        conn: Snowflake connection\n        run_id: Unique identifier for this pipeline run\n        table_name: Name of the table that was loaded\n        load_type: 'INCREMENTAL' or 'FULL_REFRESH'\n        rows_processed: Number of input rows\n        rows_inserted: Number of rows inserted\n        rows_updated: Number of rows updated\n        duration_seconds: How long the operation took\n        status: 'SUCCESS' or 'FAILED'\n        error_message: Error details if failed\n    \"\"\"\n    cur = conn.cursor()\n    \n    database = SNOWFLAKE_CONFIG['database']\n    schema = SNOWFLAKE_CONFIG['schema']\n    \n    insert_sql = f\"\"\"\n    INSERT INTO {database}.{schema}.{AUDIT_TABLE}\n    (RUN_ID, RUN_TIMESTAMP, PIPELINE_NAME, TABLE_NAME, LOAD_TYPE,\n     ROWS_PROCESSED, ROWS_INSERTED, ROWS_UPDATED, ROWS_DELETED,\n     DURATION_SECONDS, STATUS, ERROR_MESSAGE)\n    VALUES (%s, CURRENT_TIMESTAMP(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n    \"\"\"\n    \n    try:\n        cur.execute(insert_sql, (\n            run_id,\n            'SharePoint Export Pipeline',\n            table_name,\n            load_type,\n            rows_processed,\n            rows_inserted,\n            rows_updated,\n            0,  # rows_deleted (always 0 for this pipeline)\n            duration_seconds,\n            status,\n            error_message\n        ))\n        logger.info(f\"Logged run to audit table: {table_name} - {status}\")\n    except Exception as e:\n        logger.error(f\"Failed to log to audit table: {e}\")\n    finally:\n        cur.close()\n\n\nprint(\"✓ Audit logging functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4a. Audit Logging Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_incremental(conn, df: pd.DataFrame, table_name: str, match_key: str = 'ID') -> Tuple[int, int]:\n    \"\"\"\n    Load data with incremental MERGE on specified match key.\n    Returns: (rows_inserted, rows_updated)\n    \"\"\"\n    staging_table = f\"{table_name}_STAGING\"\n    cur = conn.cursor()\n    \n    database = SNOWFLAKE_CONFIG['database']\n    schema = SNOWFLAKE_CONFIG['schema']\n    \n    # Normalize dates\n    df, date_columns = normalize_dates(df)\n    \n    # Ensure target table exists with proper DATE column types\n    logger.info(f\"Ensuring target table {table_name} exists with proper schema...\")\n    create_table_with_types(conn, table_name, df, date_columns)\n    \n    # Add any new columns to existing table (schema evolution)\n    ensure_schema_matches(conn, table_name, df, date_columns)\n    \n    logger.info(f\"Creating staging table for {table_name}...\")\n    # Drop staging table if it exists\n    cur.execute(f\"DROP TABLE IF EXISTS {database}.{schema}.{staging_table};\")\n    \n    # Create staging table with same schema as target\n    create_table_with_types(conn, staging_table, df, date_columns)\n    \n    logger.info(f\"Loading {len(df)} rows into staging...\")\n    # Load data into pre-created table\n    success, nchunks, nrows, _ = write_pandas(\n        conn, df, staging_table,\n        auto_create_table=False,\n        overwrite=False\n    )\n    \n    if not success:\n        raise Exception(\"Failed to write to staging table\")\n    \n    logger.info(\"Merging data...\")\n    \n    # Build dynamic MERGE SQL\n    all_columns = df.columns.tolist()\n    update_cols = [col for col in all_columns if col != match_key]\n    \n    update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in update_cols])\n    insert_cols = \", \".join(all_columns)\n    insert_vals = \", \".join([f\"source.{col}\" for col in all_columns])\n    \n    merge_sql = f\"\"\"\n    MERGE INTO {database}.{schema}.{table_name} AS target\n    USING {database}.{schema}.{staging_table} AS source\n    ON target.{match_key} = source.{match_key}\n    WHEN MATCHED THEN\n        UPDATE SET\n        {update_set_clause}\n    WHEN NOT MATCHED THEN\n        INSERT ({insert_cols})\n        VALUES ({insert_vals});\n    \"\"\"\n    \n    cur.execute(merge_sql)\n    \n    # Get MERGE statistics from Snowflake\n    stats_query = f\"SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\"\n    cur.execute(stats_query)\n    merge_result = cur.fetchone()\n    \n    # Parse Snowflake MERGE output: [rows_inserted, rows_updated, rows_deleted]\n    rows_inserted = merge_result[0] if merge_result else 0\n    rows_updated = merge_result[1] if merge_result else 0\n    \n    logger.info(f\"Merge complete: {rows_inserted} inserted, {rows_updated} updated\")\n    \n    # Clean up staging table\n    cur.execute(f\"DROP TABLE IF EXISTS {database}.{schema}.{staging_table};\")\n    \n    cur.close()\n    logger.info(f\"Incremental load complete for {table_name}\")\n    \n    return (rows_inserted, rows_updated)\n\n\ndef load_full_refresh(conn, df: pd.DataFrame, table_name: str) -> Tuple[int, int]:\n    \"\"\"\n    Load data with full TRUNCATE and reload.\n    Skip if DataFrame is empty.\n    Returns: (rows_inserted, rows_updated)\n    \"\"\"\n    # Skip if DataFrame is empty\n    if df.empty:\n        logger.info(f\"Skipping {table_name} - no data to load\")\n        return (0, 0)\n    \n    cur = conn.cursor()\n    \n    database = SNOWFLAKE_CONFIG['database']\n    schema = SNOWFLAKE_CONFIG['schema']\n    \n    # Normalize dates\n    df, date_columns = normalize_dates(df)\n    \n    # Ensure table exists with proper DATE column types\n    logger.info(f\"Ensuring target table {table_name} exists with proper schema...\")\n    create_table_with_types(conn, table_name, df, date_columns)\n    \n    # Add any new columns to existing table (schema evolution)\n    ensure_schema_matches(conn, table_name, df, date_columns)\n    \n    logger.info(f\"Truncating {table_name} (full reload)...\")\n    cur.execute(f\"TRUNCATE TABLE {database}.{schema}.{table_name};\")\n    \n    logger.info(f\"Uploading {len(df)} rows to {table_name}...\")\n    # Load data into pre-created table\n    try:\n        success, nchunks, nrows, _ = write_pandas(\n            conn, df, table_name,\n            auto_create_table=False,\n            overwrite=False\n        )\n        logger.info(f\"Successfully uploaded {nrows} rows to {table_name}\")\n        rows_inserted = nrows\n    except Exception as e:\n        logger.error(f\"Error loading data to {table_name}: {e}\")\n        # Log first few rows for debugging\n        logger.error(f\"Sample data types: {df.dtypes}\")\n        logger.error(f\"Sample data:\\n{df.head()}\")\n        raise\n    \n    cur.close()\n    \n    # Full refresh = all inserts, no updates\n    return (rows_inserted, 0)\n\n\nprint(\"✓ Snowflake loading functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_pipeline(incremental: bool = True, dry_run: bool = False):\n    \"\"\"\n    Execute full pipeline with audit logging\n    \n    Args:\n        incremental: If True, use incremental MERGE for raw table. If False, full refresh.\n        dry_run: If True, skip Snowflake upload\n    \n    Returns:\n        df_raw, df_transformed: DataFrames for inspection\n    \"\"\"\n    logger.info(\"=\" * 80)\n    logger.info(\"STARTING SHAREPOINT EXPORT PIPELINE\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Mode: {'INCREMENTAL' if incremental else 'FULL REFRESH'}\")\n    logger.info(f\"Dry Run: {dry_run}\")\n    logger.info(\"=\" * 80)\n    \n    # Generate unique run ID for this pipeline execution\n    run_id = str(uuid.uuid4())\n    logger.info(f\"Run ID: {run_id}\")\n    \n    start_time = datetime.now()\n    conn = None\n    \n    try:\n        # ========================================================================\n        # PHASE 1: EXTRACTION\n        # ========================================================================\n        logger.info(\"PHASE 1: DATA EXTRACTION\")\n        \n        df_sharepoint = extract_sharepoint(SHAREPOINT_EXPORT_PATH)\n        df_salesforce = extract_salesforce(SALESFORCE_EXPORT_PATH)\n        \n        logger.info(f\"Extraction complete: {len(df_sharepoint)} SharePoint, {len(df_salesforce)} Salesforce records\")\n        \n        # ========================================================================\n        # PHASE 2: CLEANING\n        # ========================================================================\n        logger.info(\"PHASE 2: DATA CLEANING\")\n        \n        df_cleaned = clean_data(df_sharepoint)\n        \n        logger.info(f\"Cleaning complete: {len(df_cleaned)} records\")\n        \n        # ========================================================================\n        # PHASE 3: TRANSFORMATION\n        # ========================================================================\n        logger.info(\"PHASE 3: DATA TRANSFORMATION\")\n        \n        # Salesforce data is only used for enrichment (merged into transformed table)\n        # It is NOT loaded as a separate table to Snowflake\n        df_transformed = transform_products(df_cleaned, df_salesforce)\n        \n        logger.info(f\"Transformation complete: {len(df_transformed)} product-level records\")\n        \n        # ========================================================================\n        # PHASE 4: LOADING TO SNOWFLAKE\n        # ========================================================================\n        logger.info(\"PHASE 4: DATA LOADING\")\n        \n        if dry_run:\n            logger.info(\"[DRY RUN] Skipping Snowflake upload\")\n        else:\n            conn = get_snowflake_connection()\n            \n            try:\n                # Create audit table if it doesn't exist\n                create_audit_table(conn)\n                \n                # Load raw SharePoint data (incremental or full)\n                if incremental:\n                    logger.info(f\"Loading raw data to {SOURCE_TABLE} (INCREMENTAL)...\")\n                    load_start = datetime.now()\n                    rows_inserted, rows_updated = load_incremental(conn, df_cleaned, SOURCE_TABLE, match_key='ID')\n                    load_duration = (datetime.now() - load_start).total_seconds()\n                    \n                    # Log to audit table\n                    log_to_snowflake(\n                        conn, run_id, SOURCE_TABLE, 'INCREMENTAL',\n                        len(df_cleaned), rows_inserted, rows_updated,\n                        load_duration, 'SUCCESS'\n                    )\n                else:\n                    logger.info(f\"Loading raw data to {SOURCE_TABLE} (FULL REFRESH)...\")\n                    load_start = datetime.now()\n                    rows_inserted, rows_updated = load_full_refresh(conn, df_cleaned, SOURCE_TABLE)\n                    load_duration = (datetime.now() - load_start).total_seconds()\n                    \n                    # Log to audit table\n                    log_to_snowflake(\n                        conn, run_id, SOURCE_TABLE, 'FULL_REFRESH',\n                        len(df_cleaned), rows_inserted, rows_updated,\n                        load_duration, 'SUCCESS'\n                    )\n                \n                # Load transformed data (always full refresh for consistency)\n                # Note: This includes Salesforce enrichment columns (SALESFORCE_ID, HAS_VALUE)\n                logger.info(f\"Loading transformed data to {TARGET_TABLE} (FULL REFRESH)...\")\n                load_start = datetime.now()\n                rows_inserted, rows_updated = load_full_refresh(conn, df_transformed, TARGET_TABLE)\n                load_duration = (datetime.now() - load_start).total_seconds()\n                \n                # Log to audit table\n                log_to_snowflake(\n                    conn, run_id, TARGET_TABLE, 'FULL_REFRESH',\n                    len(df_transformed), rows_inserted, rows_updated,\n                    load_duration, 'SUCCESS'\n                )\n                \n            finally:\n                if conn:\n                    conn.close()\n                    logger.info(\"Snowflake connection closed\")\n        \n        # ========================================================================\n        # SUMMARY\n        # ========================================================================\n        elapsed = datetime.now() - start_time\n        \n        logger.info(\"=\" * 80)\n        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n        logger.info(f\"Run ID: {run_id}\")\n        logger.info(f\"Total execution time: {elapsed}\")\n        logger.info(f\"Records processed: {len(df_cleaned)} raw → {len(df_transformed)} transformed\")\n        logger.info(\"=\" * 80)\n        \n        return df_cleaned, df_transformed\n        \n    except Exception as e:\n        # Log failure to audit table if we have a connection\n        if conn and not dry_run:\n            try:\n                elapsed = (datetime.now() - start_time).total_seconds()\n                log_to_snowflake(\n                    conn, run_id, 'PIPELINE', 'FAILED',\n                    0, 0, 0, elapsed, 'FAILED',\n                    error_message=str(e)\n                )\n            except:\n                pass  # Don't fail on audit logging failure\n            finally:\n                conn.close()\n        \n        logger.error(\"=\" * 80)\n        logger.error(\"PIPELINE FAILED WITH EXCEPTION\")\n        logger.error(f\"Run ID: {run_id}\")\n        logger.error(str(e), exc_info=True)\n        logger.error(\"=\" * 80)\n        raise\n\n\nprint(\"✓ Main pipeline function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Pipeline\n",
    "\n",
    "**Run this cell to execute the full pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "# Change parameters as needed:\n",
    "#   incremental=True  -> Incremental MERGE for raw table\n",
    "#   incremental=False -> Full refresh for raw table\n",
    "#   dry_run=True      -> Skip Snowflake upload (testing only)\n",
    "\n",
    "df_raw, df_transformed = run_pipeline(incremental=True, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing & Debugging\n",
    "\n",
    "Use these cells to test individual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Extract SharePoint data\n",
    "df_test = extract_sharepoint(SHAREPOINT_EXPORT_PATH)\n",
    "print(f\"Rows: {len(df_test)}\")\n",
    "print(f\"Columns: {df_test.columns.tolist()}\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Data cleaning\n",
    "df_cleaned_test = clean_data(df_test)\n",
    "print(f\"Client type mapping sample:\")\n",
    "df_cleaned_test[['CLIENT_TYPE_DETAIL']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Product transformation\n",
    "df_trans_test = transform_products(df_cleaned_test, pd.DataFrame())\n",
    "print(f\"Product records created: {len(df_trans_test)}\")\n",
    "print(f\"Columns: {df_trans_test.columns.tolist()}\")\n",
    "df_trans_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in key columns\n",
    "print(\"Null value counts:\")\n",
    "print(f\"TITLE: {df_transformed['TITLE'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"CLIENT: {df_transformed['CLIENT'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"OVERALL_STATUS: {df_transformed['OVERALL_STATUS'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"ID: {df_transformed['ID'].isnull().sum()} / {len(df_transformed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample records\n",
    "print(\"Sample transformed records:\")\n",
    "df_transformed[['ID', 'TITLE', 'CLIENT', 'PRODUCT', 'OVERALL_STATUS']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types in final DataFrame\n",
    "print(\"Data types:\")\n",
    "df_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(f\"\\nPipeline Summary:\")\n",
    "print(f\"  Raw records: {len(df_raw)}\")\n",
    "print(f\"  Transformed records: {len(df_transformed)}\")\n",
    "print(f\"  Unique products: {df_transformed['PRODUCT'].nunique()}\")\n",
    "print(f\"  Date range: {df_transformed['REQUEST_DATE'].min()} to {df_transformed['REQUEST_DATE'].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}