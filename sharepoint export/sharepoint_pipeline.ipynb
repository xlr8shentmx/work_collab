{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SharePoint Export Pipeline\n",
    "\n",
    "ETL pipeline that:\n",
    "1. Extracts data from SharePoint CSV and Salesforce Excel exports\n",
    "2. Transforms data with cleaning, product explosion, and metrics calculation\n",
    "3. Loads data into Snowflake with incremental or full refresh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Import product mappings from separate file\n",
    "from product_mappings import PRODUCT_CONFIGS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SNOWFLAKE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    'account': \"uhgdwaas.east-us-2.azure\",\n",
    "    'user': os.getenv('SF_USERNAME'),\n",
    "    'password': os.getenv('SF_PW'),\n",
    "    'role': \"AZU_SDRP_CSZNB_PRD_DEVELOPER_ROLE\",\n",
    "    'warehouse': \"CSZNB_PRD_ANALYTICS_XS_WH\",\n",
    "    'database': 'CSZNB_PRD_OA_DEV_DB',\n",
    "    'schema': 'BASE'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE NAMES\n",
    "# ============================================================================\n",
    "\n",
    "SOURCE_TABLE = 'SHAREPOINT_ANALYTIC_REQUESTS'\n",
    "TARGET_TABLE = 'FOCUSED_ANALYTIC_REQUESTS'\n",
    "SALESFORCE_TABLE = 'SALESFORCE_INITIATIVES'\n",
    "\n",
    "# ============================================================================\n",
    "# FILE PATHS\n",
    "# ============================================================================\n",
    "\n",
    "SHAREPOINT_EXPORT_PATH = Path.home() / \"Library/CloudStorage/OneDrive-UHG/Projects/SharePoint/exports/sharepoint_requests.csv\"\n",
    "SALESFORCE_EXPORT_PATH = Path.home() / \"Library/CloudStorage/OneDrive-UHG/Projects/SharePoint/exports/salesforce_exports.xlsx\"\n",
    "\n",
    "# ============================================================================\n",
    "# BUSINESS LOGIC CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "OPEN_STATUS = ['Not Started', 'In Progress', 'Waiting']\n",
    "DAYS_ON_STATUS_THRESHOLD = 14\n",
    "\n",
    "# Client type mapping\n",
    "CLIENT_TYPE_MAPPING = {\n",
    "    '1': 'Optum Direct NBEA',\n",
    "    '2': 'Optum/UHC Cross Carrier NBEA',\n",
    "    '3': 'UHC NBEA',\n",
    "    '4': 'Opum Direct',\n",
    "    '5': 'UHC Cross Carrier',\n",
    "    '6': 'Prospective',\n",
    "    '7': 'N/A',\n",
    "    '8': 'N/A'\n",
    "}\n",
    "\n",
    "# Boolean columns\n",
    "BOOLEAN_COLUMNS = [\n",
    "    \"BARIATRIC\", \"BH\", \"CGP\", \"CSP\", \"DM\", \"KIDNEY\", \"TRANSPLANT\", \"CHD\", \"VAD\",\n",
    "    \"NICU\", \"MATERNITY\", \"FERTILITY\", \"FOCUSED_ANALYTICS\", \"OUTPATIENT_REHAB\",\n",
    "    \"OHS\", \"FCR_PROFESSIONAL\", \"CKS\", \"CKD\", \"CARDIOLOGY\", \"DME\", \"INPATIENT_REHAB\",\n",
    "    \"SPINE_PAIN_JOINT\", \"SPECIALTY_REDIRECTION\", \"MEDICAL_REBATES_ONBOARDING\",\n",
    "    \"BRS\", \"DATA_INTAKE\", \"DATA_QAVC\", \"SPECIALTY_FUSION\", \"MBO_IMPLEMENTATION\",\n",
    "    \"MSPN_IMPLEMENTATION\", \"VARIABLE_COPAY\", \"ACCUMULATOR_ADJUSTMENT\",\n",
    "    \"SMGP\", \"SGP\", \"SECOND_MD\", \"KAIA\", \"MBO_PRESALES\", \"MSPN_PRESALES\",\n",
    "    \"MEDICAL_REBATES_PREDEAL\", \"MAVEN\", \"CAR_REPORT\", \"MSK_MSS\",\n",
    "    \"FCR_FACILITY\", \"RADIATION_ONCOLOGY\", \"VIRTA_HEALTH\", \"SMO_PRESALES\",\n",
    "    \"SMO_IMPLEMENTATION\", \"SBO_HEALTH_TRUST_PRESALES\", \"SBO_HEALTH_TRUST_IMPLEMENTATION\",\n",
    "    \"CORE_SBO\", \"ENHANCE_SBO\", \"OPTUM_GUIDE\", \"CYLINDER_HEALTH\", \"RESOURCE_BRIDGE\",\n",
    "    \"PHS\", \"CANCER\", \"PODIMETRICS\"\n",
    "]\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  - Snowflake database: {SNOWFLAKE_CONFIG['database']}\")\n",
    "print(f\"  - Product configs: {len(PRODUCT_CONFIGS)} products\")\n",
    "print(f\"  - Boolean columns: {len(BOOLEAN_COLUMNS)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sharepoint(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load SharePoint CSV and return normalized DataFrame\"\"\"\n",
    "    logger.info(f\"Loading SharePoint export from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Use low_memory=False to prevent mixed type warnings\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"SharePoint file not found at {file_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.upper()\n",
    "    \n",
    "    logger.info(f\"Loaded {len(df)} SharePoint records\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_salesforce(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load Salesforce Excel export and return normalized DataFrame\"\"\"\n",
    "    logger.info(f\"Loading Salesforce export from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read Excel file (assuming first sheet)\n",
    "        df = pd.read_excel(file_path, sheet_name=0)\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Salesforce file not found at {file_path}\")\n",
    "        # Return empty DataFrame with expected columns\n",
    "        return pd.DataFrame(columns=['SALESFORCE_ID', 'HAS_VALUE'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading Salesforce file: {e}\")\n",
    "        return pd.DataFrame(columns=['SALESFORCE_ID', 'HAS_VALUE'])\n",
    "    \n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.upper()\n",
    "    \n",
    "    logger.info(f\"Loaded {len(df)} Salesforce records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and normalize raw data\"\"\"\n",
    "    logger.info(\"Cleaning and normalizing data...\")\n",
    "    \n",
    "    # Map client types\n",
    "    df['CLIENT_TYPE_DETAIL'] = (\n",
    "        df['CLIENT_TYPE_DETAIL']\n",
    "        .astype(str)\n",
    "        .map(CLIENT_TYPE_MAPPING)\n",
    "        .fillna(df['CLIENT_TYPE_DETAIL'])\n",
    "    )\n",
    "    \n",
    "    # Detect Spine Pain & Joint from product text\n",
    "    df['SPINE_PAIN_JOINT'] = df['PRODUCTS_REQUESTED'].str.contains(\n",
    "        \"Spine Pain & Joint\", case=False, na=False\n",
    "    )\n",
    "    \n",
    "    # Fill null values in boolean columns\n",
    "    df[BOOLEAN_COLUMNS] = df[BOOLEAN_COLUMNS].fillna(False)\n",
    "    \n",
    "    # Populate PRODUCTS_REQUESTED from boolean columns where null\n",
    "    mask_null = df['PRODUCTS_REQUESTED'].isnull()\n",
    "    if mask_null.any():\n",
    "        bool_array = df.loc[mask_null, BOOLEAN_COLUMNS].values\n",
    "        products_list = []\n",
    "        \n",
    "        for row_vals in bool_array:\n",
    "            if row_vals.any():\n",
    "                selected_cols = df.loc[mask_null, BOOLEAN_COLUMNS].columns[row_vals]\n",
    "                products_list.append(', '.join(selected_cols.str.title()))\n",
    "            else:\n",
    "                products_list.append('None')\n",
    "        \n",
    "        df.loc[mask_null, 'PRODUCTS_REQUESTED'] = products_list\n",
    "    \n",
    "    logger.info(\"Data cleaning complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Cleaning function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_products(df: pd.DataFrame, df_salesforce: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform wide-format data into product-level records\"\"\"\n",
    "    logger.info(\"Starting product transformation...\")\n",
    "    \n",
    "    # Explode products into separate rows\n",
    "    df_exploded = _explode_products(df)\n",
    "    \n",
    "    # Enrich with Salesforce data\n",
    "    df_enriched = _enrich_with_salesforce(df_exploded, df_salesforce)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    df_enriched = _calculate_metrics(df_enriched)\n",
    "    \n",
    "    logger.info(f\"Transformation complete: {len(df_exploded)} product-level records created\")\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "def _explode_products(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Explode wide-format data into product-level records\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Log available columns for debugging\n",
    "    logger.info(f\"Available columns in source data: {df.columns.tolist()[:20]}...\")  # First 20\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        for product_name, category, field, start_col, end_col, status_col in PRODUCT_CONFIGS:\n",
    "            # Check if this product is requested\n",
    "            if field in row.index and row[field]:\n",
    "                record = {\n",
    "                    'ID': row.get('ID'),\n",
    "                    'TITLE': row.get('TITLE'),\n",
    "                    'REQUEST_DATE': row.get('REQUEST_DATE'),\n",
    "                    'CLIENT': row.get('CLIENT'),\n",
    "                    'MARKET': row.get('MARKET'),\n",
    "                    'REQUESTOR': row.get('REQUESTOR'),\n",
    "                    'CLIENT_TYPE': row.get('CLIENT_TYPE_DETAIL'),\n",
    "                    'OVERALL_STATUS': row.get('OVERALL_STATUS'),\n",
    "                    'PRODUCTS_REQUESTED': row.get('PRODUCTS_REQUESTED'),\n",
    "                    'SALESFORCE_ID': row.get('SALESFORCE_ID'),\n",
    "                    'PRODUCT': product_name,\n",
    "                    'PRODUCT_CATEGORY': category,\n",
    "                    'START_DATE': row.get(start_col),\n",
    "                    'COMPLETE_DATE': row.get(end_col),\n",
    "                    'STATUS': row.get(status_col),\n",
    "                    'STATUS_CHANGE_DATE': row.get('STATUS_CHANGE_DATE'),\n",
    "                    'CLOSED_DATE': row.get('CLOSED_DATE'),\n",
    "                    'PTRR': row.get('PTRR')\n",
    "                }\n",
    "                records.append(record)\n",
    "    \n",
    "    df_products = pd.DataFrame(records)\n",
    "    logger.info(f\"Exploded {len(df)} requests into {len(df_products)} product records\")\n",
    "    \n",
    "    # Log sample of first record for debugging\n",
    "    if len(df_products) > 0:\n",
    "        logger.info(f\"Sample record columns: {df_products.columns.tolist()}\")\n",
    "        logger.info(f\"Sample values - TITLE: {df_products.iloc[0]['TITLE']}, CLIENT: {df_products.iloc[0]['CLIENT']}\")\n",
    "    \n",
    "    return df_products\n",
    "\n",
    "\n",
    "def _enrich_with_salesforce(df: pd.DataFrame, df_salesforce: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join with Salesforce data to enrich records\"\"\"\n",
    "    if df_salesforce is None or df_salesforce.empty:\n",
    "        logger.warning(\"No Salesforce data available for enrichment\")\n",
    "        return df\n",
    "    \n",
    "    # Normalize Salesforce column names\n",
    "    df_salesforce.columns = df_salesforce.columns.str.upper()\n",
    "    \n",
    "    # Merge on SALESFORCE_ID if available\n",
    "    if 'SALESFORCE_ID' in df.columns and 'SALESFORCE_ID' in df_salesforce.columns:\n",
    "        df = df.merge(\n",
    "            df_salesforce[['SALESFORCE_ID', 'HAS_VALUE']],\n",
    "            on='SALESFORCE_ID',\n",
    "            how='left'\n",
    "        )\n",
    "        logger.info(\"Enriched with Salesforce data\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _calculate_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate derived metrics\"\"\"\n",
    "    today = pd.Timestamp.now()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['REQUEST_DATE', 'START_DATE', 'COMPLETE_DATE',\n",
    "                   'STATUS_CHANGE_DATE', 'CLOSED_DATE']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate days open\n",
    "    df['DAYS_OPEN'] = (today - df['REQUEST_DATE']).dt.days\n",
    "    \n",
    "    # Calculate product TAT (turnaround time)\n",
    "    df['PRODUCT_TAT'] = (df['COMPLETE_DATE'] - df['START_DATE']).dt.days\n",
    "    \n",
    "    # Mark completed products\n",
    "    df['COMPLETED_PRODUCT'] = df['STATUS'].isin(['Complete', 'Completed'])\n",
    "    \n",
    "    # Extract request type and year\n",
    "    df['REQUEST_TYPE'] = df['TITLE'].str.extract(r'\\[(.*?)\\]')[0]\n",
    "    df['REQUEST_YEAR'] = df['REQUEST_DATE'].dt.year\n",
    "    \n",
    "    # Determine if product is open\n",
    "    df['PRODUCT_OPEN'] = df['STATUS'].isin(OPEN_STATUS)\n",
    "    \n",
    "    # Calculate days on current status\n",
    "    df['DAYS_ON_STATUS'] = (today - df['STATUS_CHANGE_DATE']).dt.days\n",
    "    df['DAYS_ON_STATUS'] = df['DAYS_ON_STATUS'].fillna(0).astype(int)\n",
    "    \n",
    "    # Flag items needing attention (open and on status > threshold)\n",
    "    df['NEEDS_ATTENTION'] = (\n",
    "        df['PRODUCT_OPEN'] &\n",
    "        (df['DAYS_ON_STATUS'] > DAYS_ON_STATUS_THRESHOLD)\n",
    "    )\n",
    "    \n",
    "    # Add HAS_VALUE if not present\n",
    "    if 'HAS_VALUE' not in df.columns:\n",
    "        df['HAS_VALUE'] = None\n",
    "    \n",
    "    # Generate SharePoint URL\n",
    "    df['URL'] = df['ID'].apply(\n",
    "        lambda x: f\"https://sharepoint.com/sites/analytics/Lists/Requests/DispForm.aspx?ID={x}\"\n",
    "        if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Calculated all metrics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Transformation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Snowflake Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_connection():\n",
    "    \"\"\"Establish connection to Snowflake\"\"\"\n",
    "    logger.info(\"Connecting to Snowflake...\")\n",
    "    \n",
    "    # Optional: Load private key for key-pair authentication\n",
    "    pkey_pem = os.getenv(\"MY_SF_PKEY\")\n",
    "    pkey = None\n",
    "    if pkey_pem:\n",
    "        pkey = serialization.load_pem_private_key(\n",
    "            pkey_pem.encode(\"utf-8\"),\n",
    "            password=None,\n",
    "            backend=default_backend()\n",
    "        )\n",
    "    \n",
    "    conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
    "    logger.info(\"Successfully connected to Snowflake\")\n",
    "    return conn\n",
    "\n",
    "\n",
    "def normalize_dates(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert datetime columns to string format 'YYYY-MM-DD' for reliable Snowflake DATE parsing.\n",
    "    Returns: (normalized_dataframe, list_of_date_columns)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    date_columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Check if column name ends with _DATE or contains DATE-related keywords\n",
    "        if col.endswith('_DATE') or col in ['REQUEST_DATE', 'START_DATE', 'COMPLETE_DATE',\n",
    "                                               'CLOSED_DATE', 'STATUS_CHANGE_DATE']:\n",
    "            try:\n",
    "                # Convert to datetime64 first (handles strings, floats, NaT, etc.)\n",
    "                temp_dt = pd.to_datetime(df[col], errors='coerce')\n",
    "                \n",
    "                # Convert to string format 'YYYY-MM-DD', keeping NaT as None\n",
    "                df[col] = temp_dt.apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else None)\n",
    "                \n",
    "                # Explicitly cast to object dtype to ensure write_pandas treats as VARCHAR\n",
    "                df[col] = df[col].astype('object')\n",
    "                \n",
    "                date_columns.append(col)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not convert {col} to date string: {e}\")\n",
    "    \n",
    "    if date_columns:\n",
    "        logger.info(f\"Normalized {len(date_columns)} date columns to string format\")\n",
    "    \n",
    "    return df, date_columns\n",
    "\n",
    "\n",
    "def create_table_with_types(conn, table_name: str, df: pd.DataFrame, date_columns: List[str]):\n",
    "    \"\"\"\n",
    "    Create table with explicit DATE column types for date columns.\n",
    "    Numeric columns created as NUMBER, all other columns as VARCHAR.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    database = SNOWFLAKE_CONFIG['database']\n",
    "    schema = SNOWFLAKE_CONFIG['schema']\n",
    "    \n",
    "    # Build column definitions\n",
    "    column_defs = []\n",
    "    for col in df.columns:\n",
    "        if col in date_columns:\n",
    "            column_defs.append(f\"{col} DATE\")\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check if it's an integer or float\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                column_defs.append(f\"{col} NUMBER(38,0)\")\n",
    "            else:\n",
    "                column_defs.append(f\"{col} NUMBER(38,6)\")\n",
    "        else:\n",
    "            # Use VARCHAR for all other columns\n",
    "            column_defs.append(f\"{col} VARCHAR\")\n",
    "    \n",
    "    columns_sql = \",\\n    \".join(column_defs)\n",
    "    \n",
    "    create_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database}.{schema}.{table_name} (\n",
    "        {columns_sql}\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Creating table {table_name} with {len(date_columns)} DATE columns...\")\n",
    "    \n",
    "    try:\n",
    "        cur.execute(create_sql)\n",
    "        logger.info(f\"Successfully created table {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Table creation failed (may already exist): {e}\")\n",
    "    \n",
    "    cur.close()\n",
    "\n",
    "\n",
    "print(\"✓ Snowflake connection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_incremental(conn, df: pd.DataFrame, table_name: str, match_key: str = 'ID'):\n",
    "    \"\"\"Load data with incremental MERGE on specified match key\"\"\"\n",
    "    staging_table = f\"{table_name}_STAGING\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    database = SNOWFLAKE_CONFIG['database']\n",
    "    schema = SNOWFLAKE_CONFIG['schema']\n",
    "    \n",
    "    # Normalize dates\n",
    "    df, date_columns = normalize_dates(df)\n",
    "    \n",
    "    # Ensure target table exists with proper DATE column types\n",
    "    logger.info(f\"Ensuring target table {table_name} exists with proper schema...\")\n",
    "    create_table_with_types(conn, table_name, df, date_columns)\n",
    "    \n",
    "    logger.info(f\"Creating staging table for {table_name}...\")\n",
    "    # Drop staging table if it exists\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS {database}.{schema}.{staging_table};\")\n",
    "    \n",
    "    # Create staging table with same schema as target\n",
    "    create_table_with_types(conn, staging_table, df, date_columns)\n",
    "    \n",
    "    logger.info(f\"Loading {len(df)} rows into staging...\")\n",
    "    # Load data into pre-created table\n",
    "    success, nchunks, nrows, _ = write_pandas(\n",
    "        conn, df, staging_table,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        raise Exception(\"Failed to write to staging table\")\n",
    "    \n",
    "    logger.info(\"Merging data...\")\n",
    "    \n",
    "    # Build dynamic MERGE SQL\n",
    "    all_columns = df.columns.tolist()\n",
    "    update_cols = [col for col in all_columns if col != match_key]\n",
    "    \n",
    "    update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in update_cols])\n",
    "    insert_cols = \", \".join(all_columns)\n",
    "    insert_vals = \", \".join([f\"source.{col}\" for col in all_columns])\n",
    "    \n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {database}.{schema}.{table_name} AS target\n",
    "    USING {database}.{schema}.{staging_table} AS source\n",
    "    ON target.{match_key} = source.{match_key}\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "        {update_set_clause}\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT ({insert_cols})\n",
    "        VALUES ({insert_vals});\n",
    "    \"\"\"\n",
    "    \n",
    "    result = cur.execute(merge_sql)\n",
    "    rows_affected = result.fetchone()[0] if result.rowcount > 0 else 0\n",
    "    logger.info(f\"Merge complete: {rows_affected} rows affected\")\n",
    "    \n",
    "    # Clean up staging table\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS {database}.{schema}.{staging_table};\")\n",
    "    \n",
    "    cur.close()\n",
    "    logger.info(f\"Incremental load complete for {table_name}\")\n",
    "\n",
    "\n",
    "def load_full_refresh(conn, df: pd.DataFrame, table_name: str):\n",
    "    \"\"\"Load data with full TRUNCATE and reload\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    database = SNOWFLAKE_CONFIG['database']\n",
    "    schema = SNOWFLAKE_CONFIG['schema']\n",
    "    \n",
    "    # Normalize dates\n",
    "    df, date_columns = normalize_dates(df)\n",
    "    \n",
    "    # Ensure table exists with proper DATE column types\n",
    "    logger.info(f\"Ensuring target table {table_name} exists with proper schema...\")\n",
    "    create_table_with_types(conn, table_name, df, date_columns)\n",
    "    \n",
    "    logger.info(f\"Truncating {table_name} (full reload)...\")\n",
    "    cur.execute(f\"TRUNCATE TABLE {database}.{schema}.{table_name};\")\n",
    "    \n",
    "    logger.info(f\"Uploading {len(df)} rows to {table_name}...\")\n",
    "    # Load data into pre-created table\n",
    "    success, nchunks, nrows, _ = write_pandas(\n",
    "        conn, df, table_name,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    cur.close()\n",
    "    logger.info(f\"Successfully uploaded {nrows} rows to {table_name}\")\n",
    "\n",
    "\n",
    "print(\"✓ Snowflake loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(incremental: bool = True, dry_run: bool = False):\n",
    "    \"\"\"\n",
    "    Execute full pipeline\n",
    "    \n",
    "    Args:\n",
    "        incremental: If True, use incremental MERGE for raw table. If False, full refresh.\n",
    "        dry_run: If True, skip Snowflake upload\n",
    "    \n",
    "    Returns:\n",
    "        df_raw, df_transformed: DataFrames for inspection\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"STARTING SHAREPOINT EXPORT PIPELINE\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Mode: {'INCREMENTAL' if incremental else 'FULL REFRESH'}\")\n",
    "    logger.info(f\"Dry Run: {dry_run}\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # ========================================================================\n",
    "        # PHASE 1: EXTRACTION\n",
    "        # ========================================================================\n",
    "        logger.info(\"PHASE 1: DATA EXTRACTION\")\n",
    "        \n",
    "        df_sharepoint = extract_sharepoint(SHAREPOINT_EXPORT_PATH)\n",
    "        df_salesforce = extract_salesforce(SALESFORCE_EXPORT_PATH)\n",
    "        \n",
    "        logger.info(f\"Extraction complete: {len(df_sharepoint)} SharePoint, {len(df_salesforce)} Salesforce records\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PHASE 2: CLEANING\n",
    "        # ========================================================================\n",
    "        logger.info(\"PHASE 2: DATA CLEANING\")\n",
    "        \n",
    "        df_cleaned = clean_data(df_sharepoint)\n",
    "        \n",
    "        logger.info(f\"Cleaning complete: {len(df_cleaned)} records\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PHASE 3: TRANSFORMATION\n",
    "        # ========================================================================\n",
    "        logger.info(\"PHASE 3: DATA TRANSFORMATION\")\n",
    "        \n",
    "        df_transformed = transform_products(df_cleaned, df_salesforce)\n",
    "        \n",
    "        logger.info(f\"Transformation complete: {len(df_transformed)} product-level records\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PHASE 4: LOADING TO SNOWFLAKE\n",
    "        # ========================================================================\n",
    "        logger.info(\"PHASE 4: DATA LOADING\")\n",
    "        \n",
    "        if dry_run:\n",
    "            logger.info(\"[DRY RUN] Skipping Snowflake upload\")\n",
    "        else:\n",
    "            conn = get_snowflake_connection()\n",
    "            \n",
    "            try:\n",
    "                # Load raw SharePoint data (incremental or full)\n",
    "                if incremental:\n",
    "                    logger.info(f\"Loading raw data to {SOURCE_TABLE} (INCREMENTAL)...\")\n",
    "                    load_incremental(conn, df_cleaned, SOURCE_TABLE, match_key='ID')\n",
    "                else:\n",
    "                    logger.info(f\"Loading raw data to {SOURCE_TABLE} (FULL REFRESH)...\")\n",
    "                    load_full_refresh(conn, df_cleaned, SOURCE_TABLE)\n",
    "                \n",
    "                # Load Salesforce data (always full refresh - small table)\n",
    "                logger.info(f\"Loading Salesforce data to {SALESFORCE_TABLE} (FULL REFRESH)...\")\n",
    "                load_full_refresh(conn, df_salesforce, SALESFORCE_TABLE)\n",
    "                \n",
    "                # Load transformed data (always full refresh for consistency)\n",
    "                logger.info(f\"Loading transformed data to {TARGET_TABLE} (FULL REFRESH)...\")\n",
    "                load_full_refresh(conn, df_transformed, TARGET_TABLE)\n",
    "                \n",
    "            finally:\n",
    "                conn.close()\n",
    "                logger.info(\"Snowflake connection closed\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # SUMMARY\n",
    "        # ========================================================================\n",
    "        elapsed = datetime.now() - start_time\n",
    "        \n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(f\"Total execution time: {elapsed}\")\n",
    "        logger.info(f\"Records processed: {len(df_cleaned)} raw → {len(df_transformed)} transformed\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        return df_cleaned, df_transformed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"=\" * 80)\n",
    "        logger.error(\"PIPELINE FAILED WITH EXCEPTION\")\n",
    "        logger.error(str(e), exc_info=True)\n",
    "        logger.error(\"=\" * 80)\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"✓ Main pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Pipeline\n",
    "\n",
    "**Run this cell to execute the full pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "# Change parameters as needed:\n",
    "#   incremental=True  -> Incremental MERGE for raw table\n",
    "#   incremental=False -> Full refresh for raw table\n",
    "#   dry_run=True      -> Skip Snowflake upload (testing only)\n",
    "\n",
    "df_raw, df_transformed = run_pipeline(incremental=True, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing & Debugging\n",
    "\n",
    "Use these cells to test individual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Extract SharePoint data\n",
    "df_test = extract_sharepoint(SHAREPOINT_EXPORT_PATH)\n",
    "print(f\"Rows: {len(df_test)}\")\n",
    "print(f\"Columns: {df_test.columns.tolist()}\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Data cleaning\n",
    "df_cleaned_test = clean_data(df_test)\n",
    "print(f\"Client type mapping sample:\")\n",
    "df_cleaned_test[['CLIENT_TYPE_DETAIL']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Product transformation\n",
    "df_trans_test = transform_products(df_cleaned_test, pd.DataFrame())\n",
    "print(f\"Product records created: {len(df_trans_test)}\")\n",
    "print(f\"Columns: {df_trans_test.columns.tolist()}\")\n",
    "df_trans_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in key columns\n",
    "print(\"Null value counts:\")\n",
    "print(f\"TITLE: {df_transformed['TITLE'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"CLIENT: {df_transformed['CLIENT'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"OVERALL_STATUS: {df_transformed['OVERALL_STATUS'].isnull().sum()} / {len(df_transformed)}\")\n",
    "print(f\"ID: {df_transformed['ID'].isnull().sum()} / {len(df_transformed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample records\n",
    "print(\"Sample transformed records:\")\n",
    "df_transformed[['ID', 'TITLE', 'CLIENT', 'PRODUCT', 'OVERALL_STATUS']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types in final DataFrame\n",
    "print(\"Data types:\")\n",
    "df_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(f\"\\nPipeline Summary:\")\n",
    "print(f\"  Raw records: {len(df_raw)}\")\n",
    "print(f\"  Transformed records: {len(df_transformed)}\")\n",
    "print(f\"  Unique products: {df_transformed['PRODUCT'].nunique()}\")\n",
    "print(f\"  Date range: {df_transformed['REQUEST_DATE'].min()} to {df_transformed['REQUEST_DATE'].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
