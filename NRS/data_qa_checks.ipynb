{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data Quality Assurance - Medical Claims & Membership\n\nThis notebook performs comprehensive data quality checks on source tables used in the NRS Beta 2 pipeline:\n- **FA_MEDICAL**: Medical claims data\n- **FA_MEMBERSHIP**: Member eligibility data (STAGE schema)\n\n## Checks Performed:\n1. Column existence validation\n2. NULL statistics (count and percentage)\n3. Populated statistics (count and percentage)\n4. Data type verification\n5. Sample value inspection\n6. Missing column alerts\n7. **Key field population analysis** for medical claims"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration Parameters\n",
    "CLIENT_DATA = 'EMBLEM'  # Client identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import logging\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, count, sum as ssum, when, lit\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_snowflake_session():\n",
    "    \"\"\"Create and return Snowflake session using default connection.\"\"\"\n",
    "    return Session.builder.getOrCreate()\n",
    "\n",
    "session = get_snowflake_session()\n",
    "logger.info(\"âœ“ Snowflake session created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Column Definitions\n",
    "\n",
    "Define the columns required from each source table with their criticality and purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# FA_MEDICAL expected columns\nFA_MEDICAL_COLUMNS = {\n    # Core identifiers\n    'INDV_ID': {'critical': True, 'description': 'Individual/Member ID', 'key_field': False},\n    'CLM_AUD_NBR': {'critical': True, 'description': 'Claim audit number (unique claim ID)', 'key_field': False},\n    \n    # Service dates - KEY FIELDS\n    'SRVC_FROM_DT': {'critical': True, 'description': 'Service from date', 'key_field': True},\n    'SRVC_THRU_DT': {'critical': True, 'description': 'Service through date', 'key_field': True},\n    'PROCESS_DT': {'critical': True, 'description': 'Claim process date', 'key_field': True},\n    'ADMIT_DT': {'critical': False, 'description': 'Hospital admission date', 'key_field': True},\n    'DSCHRG_DT': {'critical': False, 'description': 'Hospital discharge date', 'key_field': True},\n    \n    # Diagnosis codes - KEY FIELDS\n    'DIAG_1_CD': {'critical': True, 'description': 'Primary diagnosis code', 'key_field': True},\n    'DIAG_2_CD': {'critical': False, 'description': 'Secondary diagnosis code', 'key_field': True},\n    'DIAG_3_CD': {'critical': False, 'description': 'Tertiary diagnosis code', 'key_field': True},\n    'DIAG_4_CD': {'critical': False, 'description': 'Fourth diagnosis code', 'key_field': True},\n    'DIAG_5_CD': {'critical': False, 'description': 'Fifth diagnosis code', 'key_field': True},\n    \n    # Procedure codes - KEY FIELDS\n    'PROC_1_CD': {'critical': False, 'description': 'Primary procedure code', 'key_field': True},\n    'PROC_2_CD': {'critical': False, 'description': 'Secondary procedure code', 'key_field': True},\n    'PROC_3_CD': {'critical': False, 'description': 'Tertiary procedure code', 'key_field': True},\n    'PROC_CD': {'critical': False, 'description': 'CPT/procedure code', 'key_field': False},\n    \n    # Discharge and DRG - KEY FIELD\n    'DSCHRG_STS': {'critical': False, 'description': 'Discharge status', 'key_field': False},\n    'DRG': {'critical': False, 'description': 'Diagnosis Related Group code', 'key_field': True},\n    'DRG_TYPE': {'critical': False, 'description': 'DRG type (MS-DRG, APR-DRG)', 'key_field': False},\n    'DRG_OTLR_FLG': {'critical': False, 'description': 'DRG outlier flag', 'key_field': False},\n    'DRG_OTLR_COST': {'critical': False, 'description': 'DRG outlier cost', 'key_field': True},\n    \n    # Financial - KEY FIELDS\n    'SBMT_CHRG_AMT': {'critical': False, 'description': 'Submitted charge amount', 'key_field': True},\n    'NET_PD_AMT': {'critical': True, 'description': 'Net paid amount', 'key_field': True},\n    \n    # Service location and revenue - KEY FIELD\n    'PL_OF_SRVC_CD': {'critical': False, 'description': 'Place of service code', 'key_field': False},\n    'RVNU_CD': {'critical': False, 'description': 'Revenue code', 'key_field': True},\n    \n    # Provider information\n    'PROV_NPI': {'critical': False, 'description': 'Provider NPI', 'key_field': False},\n    'PROV_TIN': {'critical': False, 'description': 'Provider TIN', 'key_field': False},\n    'PROV_FULL_NM': {'critical': False, 'description': 'Provider full name', 'key_field': False},\n    'PROV_STATE': {'critical': False, 'description': 'Provider state', 'key_field': False},\n    'PROV_TYP_CD': {'critical': False, 'description': 'Provider type code', 'key_field': False}\n}\n\n# FA_MEMBERSHIP expected columns\nFA_MEMBERSHIP_COLUMNS = {\n    'INDV_ID': {'critical': True, 'description': 'Individual/Member ID'},\n    'GENDER': {'critical': False, 'description': 'Member gender'},\n    'BTH_DT': {'critical': True, 'description': 'Member birth date'},\n    'MEM_EFF_DT': {'critical': True, 'description': 'Member effective date'},\n    'MEM_EXP_DT': {'critical': True, 'description': 'Member expiration date'},\n    'PRDCT_CD': {'critical': False, 'description': 'Product code'},\n    'STATE': {'critical': False, 'description': 'Member state'},\n    'BUS_LINE_CD': {'critical': False, 'description': 'Business line code'}\n}\n\nprint(f\"FA_MEDICAL: {len(FA_MEDICAL_COLUMNS)} expected columns defined\")\nprint(f\"FA_MEMBERSHIP: {len(FA_MEMBERSHIP_COLUMNS)} expected columns defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def check_column_stats(session, table_name, expected_columns, sample_size=10000):\n    \"\"\"\n    Comprehensive column quality check for a Snowflake table.\n    \n    Args:\n        session: Snowpark session\n        table_name: Fully qualified table name\n        expected_columns: Dict of expected columns with metadata\n        sample_size: Number of rows to sample for analysis\n        \n    Returns:\n        dict with results including missing columns, stats, and alerts\n    \"\"\"\n    logger.info(f\"\\n{'='*80}\")\n    logger.info(f\"Analyzing table: {table_name}\")\n    logger.info(f\"{'='*80}\")\n    \n    results = {\n        'table_name': table_name,\n        'timestamp': datetime.now().isoformat(),\n        'missing_columns': [],\n        'column_stats': [],\n        'alerts': [],\n        'summary': {}\n    }\n    \n    try:\n        # Load table\n        df = session.table(table_name)\n        \n        # Get total row count\n        total_rows = df.count()\n        results['summary']['total_rows'] = total_rows\n        logger.info(f\"Total rows: {total_rows:,}\")\n        \n        if total_rows == 0:\n            results['alerts'].append({\n                'severity': 'CRITICAL',\n                'message': f'Table {table_name} is empty!'\n            })\n            return results\n        \n        # Get actual columns from table\n        actual_columns = set(df.columns)\n        expected_column_names = set(expected_columns.keys())\n        \n        # Find missing columns\n        missing = expected_column_names - actual_columns\n        if missing:\n            results['missing_columns'] = list(missing)\n            for col_name in missing:\n                col_info = expected_columns[col_name]\n                severity = 'CRITICAL' if col_info['critical'] else 'WARNING'\n                results['alerts'].append({\n                    'severity': severity,\n                    'column': col_name,\n                    'message': f\"Missing column: {col_name} - {col_info['description']}\"\n                })\n        \n        # Analyze each expected column that exists\n        for col_name, col_info in expected_columns.items():\n            if col_name not in actual_columns:\n                continue\n                \n            logger.info(f\"  Analyzing: {col_name}\")\n            \n            # Calculate NULL stats\n            null_count = df.filter(col(col_name).is_null()).count()\n            null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n            populated_count = total_rows - null_count\n            populated_pct = 100 - null_pct\n            \n            # Get data type\n            col_type = str(df.schema[col_name].datatype)\n            \n            # Get sample non-null values\n            sample_vals = (\n                df.filter(col(col_name).is_not_null())\n                  .select(col_name)\n                  .distinct()\n                  .limit(5)\n                  .to_pandas()[col_name]\n                  .tolist()\n            )\n            \n            # Get distinct count for low-cardinality columns\n            distinct_count = None\n            if populated_count > 0 and populated_count <= 100000:\n                distinct_count = df.select(col_name).distinct().count()\n            \n            col_stats = {\n                'column_name': col_name,\n                'description': col_info['description'],\n                'critical': col_info['critical'],\n                'key_field': col_info.get('key_field', False),\n                'data_type': col_type,\n                'total_rows': total_rows,\n                'null_count': null_count,\n                'null_pct': round(null_pct, 2),\n                'populated_count': populated_count,\n                'populated_pct': round(populated_pct, 2),\n                'distinct_count': distinct_count,\n                'sample_values': sample_vals\n            }\n            results['column_stats'].append(col_stats)\n            \n            # Generate alerts\n            if col_info['critical'] and null_pct > 50:\n                results['alerts'].append({\n                    'severity': 'CRITICAL',\n                    'column': col_name,\n                    'message': f\"Critical column {col_name} has {null_pct:.1f}% NULLs\"\n                })\n            elif col_info['critical'] and null_pct > 10:\n                results['alerts'].append({\n                    'severity': 'WARNING',\n                    'column': col_name,\n                    'message': f\"Critical column {col_name} has {null_pct:.1f}% NULLs\"\n                })\n            elif null_pct == 100:\n                results['alerts'].append({\n                    'severity': 'WARNING',\n                    'column': col_name,\n                    'message': f\"Column {col_name} is completely NULL\"\n                })\n        \n        # Summary statistics\n        results['summary']['expected_columns'] = len(expected_columns)\n        results['summary']['found_columns'] = len(expected_column_names & actual_columns)\n        results['summary']['missing_columns'] = len(missing)\n        results['summary']['critical_alerts'] = len([a for a in results['alerts'] if a['severity'] == 'CRITICAL'])\n        results['summary']['warnings'] = len([a for a in results['alerts'] if a['severity'] == 'WARNING'])\n        \n    except Exception as e:\n        results['alerts'].append({\n            'severity': 'CRITICAL',\n            'message': f\"Error analyzing table: {str(e)}\"\n        })\n        logger.error(f\"Error: {e}\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def print_results(results):\n",
    "    \"\"\"\n",
    "    Pretty print the QA results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QA REPORT: {results['table_name']}\")\n",
    "    print(f\"Timestamp: {results['timestamp']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    if 'summary' in results and results['summary']:\n",
    "        print(\"SUMMARY:\")\n",
    "        print(f\"  Total Rows: {results['summary'].get('total_rows', 'N/A'):,}\")\n",
    "        print(f\"  Expected Columns: {results['summary'].get('expected_columns', 0)}\")\n",
    "        print(f\"  Found Columns: {results['summary'].get('found_columns', 0)}\")\n",
    "        print(f\"  Missing Columns: {results['summary'].get('missing_columns', 0)}\")\n",
    "        print(f\"  Critical Alerts: {results['summary'].get('critical_alerts', 0)}\")\n",
    "        print(f\"  Warnings: {results['summary'].get('warnings', 0)}\")\n",
    "        print()\n",
    "    \n",
    "    # Alerts\n",
    "    if results['alerts']:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ALERTS:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for alert in sorted(results['alerts'], key=lambda x: (0 if x['severity'] == 'CRITICAL' else 1)):\n",
    "            severity = alert['severity']\n",
    "            symbol = 'ðŸ”´' if severity == 'CRITICAL' else 'âš ï¸'\n",
    "            print(f\"  {symbol} [{severity}] {alert['message']}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"âœ“ No alerts - all checks passed!\\n\")\n",
    "    \n",
    "    # Column Statistics\n",
    "    if results['column_stats']:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COLUMN STATISTICS:\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Convert to DataFrame for nice display\n",
    "        stats_df = pd.DataFrame(results['column_stats'])\n",
    "        \n",
    "        # Reorder columns for display\n",
    "        display_cols = [\n",
    "            'column_name', 'critical', 'data_type', \n",
    "            'populated_count', 'populated_pct',\n",
    "            'null_count', 'null_pct'\n",
    "        ]\n",
    "        \n",
    "        if 'distinct_count' in stats_df.columns:\n",
    "            display_cols.append('distinct_count')\n",
    "        \n",
    "        display_df = stats_df[display_cols].copy()\n",
    "        \n",
    "        # Format for display\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 30)\n",
    "        \n",
    "        print(display_df.to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        # Print sample values separately for better readability\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SAMPLE VALUES:\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        for _, row in stats_df.iterrows():\n",
    "            samples = row['sample_values'][:3]  # Show first 3 samples\n",
    "            samples_str = ', '.join([str(s) for s in samples]) if samples else 'No samples'\n",
    "            print(f\"  {row['column_name']:30} {samples_str}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def export_results_to_csv(results, output_file):\n",
    "    \"\"\"\n",
    "    Export QA results to CSV file.\n",
    "    \"\"\"\n",
    "    if results['column_stats']:\n",
    "        df = pd.DataFrame(results['column_stats'])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"âœ“ Results exported to {output_file}\")\n",
    "    else:\n",
    "        logger.warning(\"No column statistics to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run QA Checks - FA_MEDICAL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Check FA_MEDICAL table\nfa_medical_table = f\"FA_MEDICAL_{CLIENT_DATA}\"\nfa_medical_results = check_column_stats(\n    session, \n    fa_medical_table, \n    FA_MEDICAL_COLUMNS,\n    sample_size=10000\n)\n\n# Print results\nprint_results(fa_medical_results)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Field Population Analysis - FA_MEDICAL\n\nDetailed analysis of population rates for key fields used in NRS pipeline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Extract and highlight key field statistics\nif fa_medical_results['column_stats']:\n    key_fields_df = pd.DataFrame([\n        stat for stat in fa_medical_results['column_stats'] \n        if stat.get('key_field', False)\n    ])\n    \n    if not key_fields_df.empty:\n        print(f\"\\n{'='*80}\")\n        print(\"KEY FIELD POPULATION RATES - FA_MEDICAL\")\n        print(f\"{'='*80}\\n\")\n        \n        # Group by category\n        date_fields = ['SRVC_FROM_DT', 'SRVC_THRU_DT', 'PROCESS_DT', 'ADMIT_DT', 'DSCHRG_DT']\n        diag_fields = ['DIAG_1_CD', 'DIAG_2_CD', 'DIAG_3_CD', 'DIAG_4_CD', 'DIAG_5_CD']\n        proc_fields = ['PROC_1_CD', 'PROC_2_CD', 'PROC_3_CD']\n        amount_fields = ['SBMT_CHRG_AMT', 'NET_PD_AMT', 'DRG_OTLR_COST']\n        code_fields = ['DRG', 'RVNU_CD']\n        \n        categories = [\n            ('DATE FIELDS', date_fields),\n            ('DIAGNOSIS CODES', diag_fields),\n            ('PROCEDURE CODES', proc_fields),\n            ('AMOUNT FIELDS', amount_fields),\n            ('CODE FIELDS', code_fields)\n        ]\n        \n        for cat_name, field_list in categories:\n            cat_df = key_fields_df[key_fields_df['column_name'].isin(field_list)]\n            if not cat_df.empty:\n                print(f\"\\n{cat_name}:\")\n                print(\"-\" * 80)\n                for _, row in cat_df.iterrows():\n                    pop_pct = row['populated_pct']\n                    status = 'âœ“' if pop_pct >= 90 else 'âš ï¸' if pop_pct >= 50 else 'âœ—'\n                    print(f\"  {status} {row['column_name']:20} {pop_pct:6.2f}% populated ({row['populated_count']:,} rows)\")\n        \n        print(f\"\\n{'='*80}\\n\")\n        \n        # Summary statistics\n        print(\"SUMMARY BY CATEGORY:\")\n        print(\"-\" * 80)\n        for cat_name, field_list in categories:\n            cat_df = key_fields_df[key_fields_df['column_name'].isin(field_list)]\n            if not cat_df.empty:\n                avg_pop = cat_df['populated_pct'].mean()\n                min_pop = cat_df['populated_pct'].min()\n                max_pop = cat_df['populated_pct'].max()\n                print(f\"  {cat_name:25} Avg: {avg_pop:6.2f}%  Min: {min_pop:6.2f}%  Max: {max_pop:6.2f}%\")\n        print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run QA Checks - FA_MEMBERSHIP Table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Check FA_MEMBERSHIP table\nfa_membership_table = f\"STAGE.FA_MEMBERSHIP_{CLIENT_DATA}\"\nfa_membership_results = check_column_stats(\n    session, \n    fa_membership_table, \n    FA_MEMBERSHIP_COLUMNS,\n    sample_size=10000\n)\n\n# Print results\nprint_results(fa_membership_results)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def print_combined_summary(fa_results, fa_membership_results):\n    \"\"\"\n    Print a combined summary of both table checks.\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(\"COMBINED DATA QA SUMMARY\")\n    print(f\"{'='*80}\\n\")\n    \n    tables = [\n        ('FA_MEDICAL', fa_results),\n        ('PS_MEMBERSHIP', fa_membership_results)\n    ]\n    \n    summary_data = []\n    all_critical_alerts = []\n    all_warnings = []\n    \n    for table_name, results in tables:\n        if 'summary' in results and results['summary']:\n            summary_data.append({\n                'Table': table_name,\n                'Total Rows': f\"{results['summary'].get('total_rows', 0):,}\",\n                'Expected Cols': results['summary'].get('expected_columns', 0),\n                'Found Cols': results['summary'].get('found_columns', 0),\n                'Missing Cols': results['summary'].get('missing_columns', 0),\n                'Critical Alerts': results['summary'].get('critical_alerts', 0),\n                'Warnings': results['summary'].get('warnings', 0)\n            })\n            \n            # Collect critical alerts\n            for alert in results['alerts']:\n                if alert['severity'] == 'CRITICAL':\n                    all_critical_alerts.append(f\"{table_name}: {alert['message']}\")\n                else:\n                    all_warnings.append(f\"{table_name}: {alert['message']}\")\n    \n    # Display summary table\n    summary_df = pd.DataFrame(summary_data)\n    print(summary_df.to_string(index=False))\n    print()\n    \n    # Display critical alerts\n    if all_critical_alerts:\n        print(f\"\\nðŸ”´ CRITICAL ALERTS ({len(all_critical_alerts)}):\")\n        for alert in all_critical_alerts:\n            print(f\"  â€¢ {alert}\")\n    \n    # Display warnings\n    if all_warnings:\n        print(f\"\\nâš ï¸  WARNINGS ({len(all_warnings)}):\")\n        for warning in all_warnings[:10]:  # Show first 10\n            print(f\"  â€¢ {warning}\")\n        if len(all_warnings) > 10:\n            print(f\"  ... and {len(all_warnings) - 10} more warnings\")\n    \n    # Overall status\n    print(f\"\\n{'='*80}\")\n    if not all_critical_alerts:\n        print(\"âœ“ Overall Status: PASSED (no critical issues)\")\n    else:\n        print(f\"âœ— Overall Status: FAILED ({len(all_critical_alerts)} critical issues)\")\n    print(f\"{'='*80}\\n\")\n\nprint_combined_summary(fa_medical_results, ps_membership_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "print_combined_summary(fa_medical_results, fa_membership_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Export combined results to JSON for programmatic access\nimport json\n\ncombined_results = {\n    'timestamp': datetime.now().isoformat(),\n    'client': CLIENT_DATA,\n    'tables': {\n        'FA_MEDICAL': fa_medical_results,\n        'FA_MEMBERSHIP': fa_membership_results\n    }\n}\n\noutput_file = f'qa_combined_{CLIENT_DATA}.json'\nwith open(output_file, 'w') as f:\n    json.dump(combined_results, f, indent=2, default=str)\n\nlogger.info(f\"âœ“ Combined results exported to {output_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Based on the QA results above:\n",
    "\n",
    "### Critical Issues (Must Fix)\n",
    "- Any missing critical columns must be added to source tables\n",
    "- Critical columns with >50% NULLs need data quality investigation\n",
    "\n",
    "### Warnings (Should Review)\n",
    "- Non-critical columns with high NULL rates may impact specific analyses\n",
    "- Completely NULL columns should be investigated or removed\n",
    "\n",
    "### Next Steps\n",
    "1. Review all critical alerts and coordinate with data engineering team\n",
    "2. Investigate columns with unexpected NULL rates\n",
    "3. Validate sample values match expected data formats\n",
    "4. Re-run QA after any data corrections\n",
    "5. Schedule regular QA checks (e.g., monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Baby DRG Frequency Analysis\n\nIdentify newborn claims and analyze DRG code utilization patterns. This analysis:\n- Identifies babies using revenue codes (REF_NEWBORN_REVCODE) and diagnosis codes (REF_NEWBORN_ICD)\n- Calculates DRG frequency by distinct member count\n- Shows date ranges for each DRG code\n\nThis helps determine which DRG codes are most commonly associated with newborn care.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Baby DRG Frequency Analysis\nfrom snowflake.snowpark.functions import count_distinct, min as sfmin, max as sfmax\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"BABY DRG FREQUENCY ANALYSIS\")\nlogger.info(\"=\"*80)\n\n# Initialize baby_drg_results as None\nbaby_drg_results = None\n\ntry:\n    # Load FA_MEDICAL table\n    fa_medical_table = f\"FA_MEDICAL_{CLIENT_DATA}\"\n    df = session.table(fa_medical_table)\n    \n    logger.info(f\"Loading reference tables for newborn identification...\")\n    \n    # Load reference codes for newborn identification\n    rev_codes = session.table(\"SUPP_DATA.REF_NEWBORN_REVCODE\").select(\"CODE\").to_pandas()['CODE'].astype(str).tolist()\n    icd_codes = session.table(\"SUPP_DATA.REF_NEWBORN_ICD\").select(\"CODE\").to_pandas()['CODE'].astype(str).tolist()\n    \n    logger.info(f\"  - Newborn revenue codes: {len(rev_codes)} codes\")\n    logger.info(f\"  - Newborn ICD codes: {len(icd_codes)} codes\")\n    \n    # Identify baby claims using revenue codes and diagnosis codes\n    logger.info(f\"\\nIdentifying baby claims...\")\n    \n    baby_claims = df.filter(\n        # Revenue code match\n        col(\"RVNU_CD\").cast(\"string\").isin(rev_codes) |\n        # OR any diagnosis code match (DIAG_1_CD through DIAG_5_CD)\n        col(\"DIAG_1_CD\").cast(\"string\").isin(icd_codes) |\n        col(\"DIAG_2_CD\").cast(\"string\").isin(icd_codes) |\n        col(\"DIAG_3_CD\").cast(\"string\").isin(icd_codes) |\n        col(\"DIAG_4_CD\").cast(\"string\").isin(icd_codes) |\n        col(\"DIAG_5_CD\").cast(\"string\").isin(icd_codes)\n    ).select(\n        \"INDV_ID\",\n        \"SRVC_FROM_DT\", \n        \"DRG\",\n        \"RVNU_CD\",\n        \"DIAG_1_CD\",\n        \"DIAG_2_CD\",\n        \"DIAG_3_CD\",\n        \"DIAG_4_CD\",\n        \"DIAG_5_CD\"\n    )\n    \n    # Count total baby claims\n    total_baby_claims = baby_claims.count()\n    logger.info(f\"  Found {total_baby_claims:,} baby claims\")\n    \n    if total_baby_claims == 0:\n        print(\"\\nNo baby claims found. Check if reference tables are populated.\")\n    else:\n        # Analyze DRG frequency for baby claims\n        logger.info(f\"\\nCalculating DRG frequency statistics...\")\n        \n        drg_freq = baby_claims.filter(\n            col(\"DRG\").is_not_null()\n        ).group_by(\"DRG\").agg(\n            count_distinct(\"INDV_ID\").alias(\"MEMBER_COUNT\"),\n            sfmin(\"SRVC_FROM_DT\").alias(\"MIN_DATE\"),\n            sfmax(\"SRVC_FROM_DT\").alias(\"MAX_DATE\")\n        ).order_by(\n            col(\"MEMBER_COUNT\").desc()\n        )\n        \n        # Convert to pandas for display and export\n        baby_drg_results = drg_freq.to_pandas()\n        \n        # Display results\n        print(f\"\\n{'='*80}\")\n        print(\"DRG CODE FREQUENCY FOR NEWBORN CLAIMS\")\n        print(f\"{'='*80}\\n\")\n        print(f\"Total Baby Claims: {total_baby_claims:,}\")\n        print(f\"Unique DRG Codes: {len(baby_drg_results):,}\")\n        print(f\"Unique Members: {baby_claims.select('INDV_ID').distinct().count():,}\")\n        print()\n        \n        # Display top DRG codes\n        print(\"TOP DRG CODES BY MEMBER COUNT:\")\n        print(\"-\" * 80)\n        pd.set_option('display.max_rows', 50)\n        pd.set_option('display.width', None)\n        pd.set_option('display.max_colwidth', None)\n        \n        # Format the dataframe for display\n        display_df = baby_drg_results.copy()\n        display_df['MIN_DATE'] = pd.to_datetime(display_df['MIN_DATE']).dt.strftime('%Y-%m-%d')\n        display_df['MAX_DATE'] = pd.to_datetime(display_df['MAX_DATE']).dt.strftime('%Y-%m-%d')\n        \n        print(display_df.to_string(index=False))\n        print()\n        \n        # Summary statistics\n        print(f\"\\n{'='*80}\")\n        print(\"SUMMARY STATISTICS:\")\n        print(\"-\" * 80)\n        print(f\"  Total DRG codes: {len(baby_drg_results)}\")\n        print(f\"  Average members per DRG: {baby_drg_results['MEMBER_COUNT'].mean():.1f}\")\n        print(f\"  Median members per DRG: {baby_drg_results['MEMBER_COUNT'].median():.1f}\")\n        print(f\"  Most common DRG: {baby_drg_results.iloc[0]['DRG']} ({baby_drg_results.iloc[0]['MEMBER_COUNT']} members)\")\n        print(f\"  Date range: {display_df['MIN_DATE'].min()} to {display_df['MAX_DATE'].max()}\")\n        print(f\"{'='*80}\\n\")\n        \nexcept Exception as e:\n    logger.error(f\"Error in baby DRG frequency analysis: {e}\")\n    print(f\"\\nError: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Consolidated Excel Export\n\nExport all QA results to a single Excel file with multiple tabs for easy analysis and sharing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Export all results to a single Excel file with multiple tabs\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"CONSOLIDATED EXCEL EXPORT\")\nlogger.info(\"=\"*80)\n\ntry:\n    output_file = f'data_qa_results_{CLIENT_DATA}.xlsx'\n    \n    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n        # Tab 1: FA_MEDICAL Column Statistics\n        if fa_medical_results and fa_medical_results.get('column_stats'):\n            medical_df = pd.DataFrame(fa_medical_results['column_stats'])\n            # Drop sample_values column for cleaner export\n            if 'sample_values' in medical_df.columns:\n                medical_df = medical_df.drop('sample_values', axis=1)\n            medical_df.to_excel(writer, sheet_name='FA_MEDICAL_Checks', index=False)\n            logger.info(f\"  âœ“ Exported FA_MEDICAL checks ({len(medical_df)} columns)\")\n        \n        # Tab 2: FA_MEMBERSHIP Column Statistics\n        if fa_membership_results and fa_membership_results.get('column_stats'):\n            membership_df = pd.DataFrame(fa_membership_results['column_stats'])\n            # Drop sample_values column for cleaner export\n            if 'sample_values' in membership_df.columns:\n                membership_df = membership_df.drop('sample_values', axis=1)\n            membership_df.to_excel(writer, sheet_name='FA_MEMBERSHIP_Checks', index=False)\n            logger.info(f\"  âœ“ Exported FA_MEMBERSHIP checks ({len(membership_df)} columns)\")\n        \n        # Tab 3: Baby DRG Frequency\n        if baby_drg_results is not None and not baby_drg_results.empty:\n            # Format dates for export\n            baby_export_df = baby_drg_results.copy()\n            baby_export_df['MIN_DATE'] = pd.to_datetime(baby_export_df['MIN_DATE'])\n            baby_export_df['MAX_DATE'] = pd.to_datetime(baby_export_df['MAX_DATE'])\n            baby_export_df.to_excel(writer, sheet_name='Baby_DRG_Frequency', index=False)\n            logger.info(f\"  âœ“ Exported Baby DRG frequency ({len(baby_export_df)} DRG codes)\")\n        \n        # Tab 4: Summary\n        summary_data = []\n        \n        if fa_medical_results and fa_medical_results.get('summary'):\n            summary_data.append({\n                'Table': 'FA_MEDICAL',\n                'Total_Rows': fa_medical_results['summary'].get('total_rows', 0),\n                'Expected_Columns': fa_medical_results['summary'].get('expected_columns', 0),\n                'Found_Columns': fa_medical_results['summary'].get('found_columns', 0),\n                'Missing_Columns': fa_medical_results['summary'].get('missing_columns', 0),\n                'Critical_Alerts': fa_medical_results['summary'].get('critical_alerts', 0),\n                'Warnings': fa_medical_results['summary'].get('warnings', 0)\n            })\n        \n        if fa_membership_results and fa_membership_results.get('summary'):\n            summary_data.append({\n                'Table': 'FA_MEMBERSHIP',\n                'Total_Rows': fa_membership_results['summary'].get('total_rows', 0),\n                'Expected_Columns': fa_membership_results['summary'].get('expected_columns', 0),\n                'Found_Columns': fa_membership_results['summary'].get('found_columns', 0),\n                'Missing_Columns': fa_membership_results['summary'].get('missing_columns', 0),\n                'Critical_Alerts': fa_membership_results['summary'].get('critical_alerts', 0),\n                'Warnings': fa_membership_results['summary'].get('warnings', 0)\n            })\n        \n        if baby_drg_results is not None and not baby_drg_results.empty:\n            summary_data.append({\n                'Table': 'Baby_DRG_Analysis',\n                'Total_Rows': len(baby_drg_results),\n                'Expected_Columns': 3,\n                'Found_Columns': 3,\n                'Missing_Columns': 0,\n                'Critical_Alerts': 0,\n                'Warnings': 0\n            })\n        \n        if summary_data:\n            summary_df = pd.DataFrame(summary_data)\n            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n            logger.info(f\"  âœ“ Exported summary tab\")\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"âœ“ All QA results exported to: {output_file}\")\n    print(f\"{'='*80}\")\n    print(\"\\nExcel file contains the following tabs:\")\n    print(\"  1. FA_MEDICAL_Checks - Column statistics for medical claims\")\n    print(\"  2. FA_MEMBERSHIP_Checks - Column statistics for membership data\")\n    print(\"  3. Baby_DRG_Frequency - DRG utilization for newborn claims\")\n    print(\"  4. Summary - High-level summary of all checks\")\n    print(f\"{'='*80}\\n\")\n    \nexcept Exception as e:\n    logger.error(f\"Error creating consolidated Excel file: {e}\")\n    print(f\"\\nError: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}